{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Results\n",
    "\n",
    "This notebook reads the results from each of the finalized shapefiles and writes the results to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import zipfile\n",
    "\n",
    "# set working dir\n",
    "HOME = op.join(op.expanduser(\"~\"))\n",
    "os.chdir(os.path.join(HOME, \"git/wgms-glacier-project\"))\n",
    "\n",
    "# Set glacier and ice catchment region numbers\n",
    "region = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', \n",
    "          '12', '13', '14', '15', '16', '17', '18', '19']\n",
    "#catch_region = ['3', '4', '5', '6', '7', '8', '9', '10', '17', '19']\n",
    "\n",
    "# To include the clipped ice caps set this to 1 to exclude them set to 0 \n",
    "# Note as of Aug 2021, decided to not use the clipped catchments\n",
    "include_clipped = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the glacier shapefiles\n",
    "for x in region:\n",
    "    # Read Region 19 separately because they have a different naming convention for mainland and islands\n",
    "    if x == \"19\":\n",
    "        # Unzip mainland glacier file\n",
    "        mainland_glacier_fp = \"data/final-dataset/region-\" + x + \"-mainland-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(mainland_glacier_fp,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        # Unxip island glacier file\n",
    "        island_glacier_fp = \"data/final-dataset/region-\" + x + \"-islands-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(island_glacier_fp,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "    else:\n",
    "        # Unzip all other region glacier files\n",
    "        glacier_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(glacier_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized glacier shapefiles and concatenate them to a single data frame\n",
    "for x in region:\n",
    "    # Read the first region\n",
    "    if x == '1':\n",
    "        glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "        glacier_regions = gpd.read_file(glacier_shapefile_fn)\n",
    "\n",
    "    # Read Regioin 19 files with the different naming convention\n",
    "    elif x == \"19\":\n",
    "        # Read mainland glacier file\n",
    "        mainland_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \\\n",
    "        \"-mainland-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(mainland_glacier_shapefile)\n",
    "        glacier_regions_part['reg_name'] = 'Antarctic Mainland'\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)\n",
    "        \n",
    "        # Read island glacier file\n",
    "        island_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \\\n",
    "        \"-islands-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(island_glacier_shapefile)\n",
    "        glacier_regions_part['reg_name'] = 'Subantarctic Islands'\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)  \n",
    "   \n",
    "    # Read all the other regions\n",
    "    else:\n",
    "        glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(glacier_shapefile_fn)\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final-dataset/compiled-glacier-sizes.csv already extists\n"
     ]
    }
   ],
   "source": [
    "# Write glacier dataframe to csv\n",
    "csv_glacier_fp = \"data/final-dataset/compiled-glacier-sizes.csv\"\n",
    "if os.path.exists(csv_glacier_fp) == False:\n",
    "    glacier_regions.to_csv(csv_glacier_fp, encoding='utf-8-sig', \n",
    "                           index=False, columns=['region_no', 'reg_name', 'glac_name', \n",
    "                                                 'glims_id', 'rgi_id', 'primeclass', 'area_km2', 'area_src', 'date'])\n",
    "    print(\"Creating csv file: \" + csv_glacier_fp)\n",
    "else:\n",
    "    print(csv_glacier_fp + \" already extists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the ice catchment shapefiles\n",
    "for x in region:\n",
    "    # Set up special case for Region 19 - Antarctica with the different naming convention for mainland and islands\n",
    "    # and for the inclusion of clipped ice caps\n",
    "    if x == \"19\":\n",
    "        # Unzip mainland ice cap file\n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-mainland-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "\n",
    "        # Unzip island ice cap file\n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-islands-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        # Unzip clipped file\n",
    "        if include_clipped == 1:\n",
    "            ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-islands-largest-complexes-clipped.zip\"\n",
    "        \n",
    "            with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "                zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "                \n",
    "    # Special statemnet for regions 5, 7, and 9 which have clipped versions of the ice caps\n",
    "    elif x == \"5\" or x == \"7\" or x == \"9\":\n",
    "        # Unzip regular ice cap files for these regions\n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        # Unzip the clipped ice cap files for these regions\n",
    "        if include_clipped == 1:\n",
    "            ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes-clipped.zip\"\n",
    "        \n",
    "            with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "                zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "                \n",
    "    # Unzip all other regions ice cap files\n",
    "    else: \n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized ice catchment shapefiles and concatenate them to a single data frame\n",
    "for x in region:\n",
    "    ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes.shp\"\n",
    "    \n",
    "    # Read first shapefile to set up dataframe\n",
    "    if x == \"1\":\n",
    "        ic_regions = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions['clipped'] = 0\n",
    "        \n",
    "    # Read region 19 shapefile\n",
    "    elif x == \"19\":\n",
    "        # Read mainland ice cap file\n",
    "        ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-mainland-largest-complexes.shp\"\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions_part['reg_name'] = 'Antarctic Mainland'\n",
    "        ic_regions = ic_regions.append(ic_regions_part)\n",
    "        \n",
    "        # Read island ice cap file\n",
    "        ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-islands-largest-complexes.shp\"\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions_part['reg_name'] = 'Subantarctic Islands'\n",
    "        ic_regions = ic_regions.append(ic_regions_part)\n",
    "        \n",
    "        # Read island clipped ice cap file\n",
    "        if include_clipped == 1:\n",
    "            ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-islands-largest-complexes-clipped.shp\"\n",
    "            ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "            ic_regions_part['clipped'] = 1\n",
    "            # drop rgi_ids column\n",
    "            #ic_regions_part = ic_regions_rgi.drop(['rgi_ids'], axis=1)\n",
    "            ic_regions = ic_regions.append(ic_regions_part)\n",
    "    \n",
    "    # Read regions 5, 7, and 9 and the clipped versions as well if desired\n",
    "    elif x == \"5\" or x == \"7\" or x == \"9\":\n",
    "        # Read the regular ice cap files for these regions\n",
    "        ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes.shp\"\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions = ic_regions.append(ic_regions_part)\n",
    "        \n",
    "        # Read the clipped versions of these ice cap files for these regions\n",
    "        if include_clipped == 1:\n",
    "            ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes-clipped.shp\"        \n",
    "            ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "            ic_regions_part['clipped'] = 1\n",
    "            ic_regions = ic_regions.append(ic_regions_part)\n",
    "\n",
    "    # Read all the other region shapefiles\n",
    "    else:\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions = ic_regions.append(ic_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating csv file: data/final-dataset/compiled-complex-sizes.csv\n"
     ]
    }
   ],
   "source": [
    "# Write ice complex dataframe to csv\n",
    "csv_catchment_fp = \"data/final-dataset/compiled-complex-sizes.csv\"\n",
    "if os.path.exists(csv_catchment_fp) == False:\n",
    "    ic_regions.to_csv(csv_catchment_fp, encoding='utf-8-sig', index=False, columns=['region_no', 'reg_name', \n",
    "                                                                                    'ic_name', 'primeclass', \n",
    "                                                                                    'area_km2', 'min_date',\n",
    "                                                                                    'max_date'])#, 'clipped'])\n",
    "                                                                                    \n",
    "    print(\"Creating csv file: \" + csv_catchment_fp)\n",
    "else:\n",
    "    print(csv_catchment_fp + \" already extists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unzipped files to save disk space\n",
    "filelist = glob.glob(\"data/final-dataset/unzipped/*\")\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
