{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Results\n",
    "\n",
    "This notebook reads the results from each of the finalized shapefiles and writes the results to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import zipfile\n",
    "\n",
    "# set working dir\n",
    "HOME = op.join(op.expanduser(\"~\"))\n",
    "os.chdir(os.path.join(HOME, \"git/wgms-glacier-project\"))\n",
    "\n",
    "# Set glacier and ice catchment region numbers\n",
    "region = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n",
    "catch_region = ['3', '4', '5', '6', '7', '8', '9', '10', '17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the glacier shapefiles\n",
    "for x in region:\n",
    "    glacier_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-glaciers.zip\"\n",
    "    with zipfile.ZipFile(glacier_zipfile_fn,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "        \n",
    "# This section no longer needed becuase not spliting up region 7, may add it back for region 19 though. TBD\n",
    "#    if x == \"7\":\n",
    "#        # Unzip region 7 separately because its file naming convention is different\n",
    "#        jan_mayen_glacier_fp = \"data/final-dataset/region-\" + x + \"-jan-mayen-largest-glaciers.zip\"\n",
    "#        with zipfile.ZipFile(jan_mayen_glacier_fp,\"r\") as zip_ref:\n",
    "#            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "#            \n",
    "#        svalbard_glacier_fp = \"data/final-dataset/region-\" + x + \"-svalbard-largest-glaciers.zip\"\n",
    "#        with zipfile.ZipFile(svalbard_glacier_fp,\"r\") as zip_ref:\n",
    "#            zip_ref.extractall(\"data/final-dataset/unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized glacier shapefiles and concatenate them to a single data frame\n",
    "for x in region:\n",
    "    glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "    if x == '1'\n",
    "        glacier_regions = gpd.read_file(glacier_shapefile_fn)\n",
    "\n",
    "# This section no longer needed becuase not spliting up region 7, may add it back for region 19 though. TBD\n",
    "#    elif x == \"7\":\n",
    "#        jan_mayen_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-jan-mayen-largest-glaciers.shp\"\n",
    "#        glacier_regions_part = gpd.read_file(jan_mayen_glacier_shapefile)\n",
    "#        glacier_regions = glacier_regions.append(glacier_regions_part)\n",
    "#        \n",
    "#        svalbard_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-svalbard-largest-glaciers.shp\"\n",
    "#        glacier_regions_part = gpd.read_file(svalbard_glacier_shapefile)\n",
    "#        glacier_regions = glacier_regions.append(glacier_regions_part)  \n",
    "#        \n",
    "    else:\n",
    "        glacier_regions_part = gpd.read_file(glacier_shapefile_fn)\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write glacier dataframe to csv\n",
    "csv_glacier_fp = \"data/final-dataset/compiled-glacier-sizes.csv\"\n",
    "glacier_regions.to_csv(csv_glacier_fp, encoding='utf-8-sig', index=False, columns=['region_no', 'reg_name', 'glac_name', \n",
    "                                                                                   'glims_id', 'primeclass', 'area_km2', \n",
    "                                                                                   'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the ice catchment shapefiles\n",
    "for x in catch_region:\n",
    "    ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes.zip\"\n",
    "    with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "        \n",
    "# This section no longer needed becuase not spliting up region 7, may add it back for region 19 though. TBD\n",
    "#    if x == \"7\":\n",
    "#        # Unzip region 7 separately because its file naming convention is different\n",
    "#        jan_mayen_ic_fp = \"data/final-dataset/region-\" + x + \"-jan-mayen-largest-ice-caps.zip\"\n",
    "#        with zipfile.ZipFile(jan_mayen_ic_fp,\"r\") as zip_ref:\n",
    "#            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "#            \n",
    "#        svalbard_ic_fp = \"data/final-dataset/region-\" + x + \"-svalbard-largest-ice-caps.zip\"\n",
    "#        with zipfile.ZipFile(svalbard_ic_fp,\"r\") as zip_ref:\n",
    "#            zip_ref.extractall(\"data/final-dataset/unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized ice catchment shapefiles and concatenate them to a single data frame\n",
    "for x in catch_region:\n",
    "    ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes.shp\"\n",
    "    if x == \"3\":\n",
    "        ic_regions = gpd.read_file(ic_shapefile_fn)\n",
    "\n",
    "# This section no longer needed becuase not spliting up region 7, may add it back for region 19 though. TBD\n",
    "#    elif x == \"7\":\n",
    "#        jan_mayen_ic_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-jan-mayen-largest-ice-caps.shp\"\n",
    "#        ic_regions_part = gpd.read_file(jan_mayen_ic_shapefile)\n",
    "#        ic_regions = ic_regions.append(ic_regions_part)\n",
    "#        \n",
    "#        svalbard_ic_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-svalbard-largest-ice-caps.shp\"\n",
    "#        ic_regions_part = gpd.read_file(svalbard_ic_shapefile)\n",
    "#        ic_regions = ic_regions.append(ic_regions_part)  \n",
    "        \n",
    "    else:\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions = ic_regions.append(ic_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write glacier dataframe to csv\n",
    "csv_fp = \"data/final-dataset/compiled-catchment-sizes.csv\"\n",
    "ic_regions.to_csv(csv_fp, encoding='utf-8-sig', index=False, columns=['region_no', 'reg_name', 'ic_name', \n",
    "                                                'primeclass', 'area_km2', 'min_date', 'max_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unzipped files to save disk space\n",
    "filelist = glob.glob(\"data/final-dataset/unzipped/*\")\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
