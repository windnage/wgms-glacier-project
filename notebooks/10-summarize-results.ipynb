{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Results\n",
    "\n",
    "This notebook reads the results from each of the finalized shapefiles and writes the results to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import zipfile\n",
    "\n",
    "# set working dir\n",
    "HOME = op.join(op.expanduser(\"~\"))\n",
    "os.chdir(os.path.join(HOME, \"git/wgms-glacier-project\"))\n",
    "\n",
    "# Set glacier and ice catchment region numbers\n",
    "region = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', \n",
    "          '12', '13', '14', '15', '16', '17', '18', '19']\n",
    "#catch_region = ['3', '4', '5', '6', '7', '8', '9', '10', '17', '19']\n",
    "\n",
    "# To include the clipped ice caps set this to 1 to exclude them set to 0 \n",
    "# Note as of Aug 2021, decided to not use the clipped catchments\n",
    "include_clipped = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the glacier shapefiles\n",
    "for x in region:\n",
    "    # Read Region 19 separately because they have a different naming convention for mainland and islands\n",
    "    if x == \"19\":\n",
    "        # Unzip mainland glacier file\n",
    "        mainland_glacier_fp = \"data/final-dataset/region-\" + x + \"-mainland-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(mainland_glacier_fp,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        # Unxip island glacier file\n",
    "        island_glacier_fp = \"data/final-dataset/region-\" + x + \"-islands-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(island_glacier_fp,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "    else:\n",
    "        # Unzip all other region glacier files\n",
    "        glacier_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(glacier_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized glacier shapefiles and concatenate them to a single data frame\n",
    "for x in region:\n",
    "    # Read the first region\n",
    "    if x == '1':\n",
    "        glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "        glacier_regions = gpd.read_file(glacier_shapefile_fn)\n",
    "\n",
    "    # Read Regioin 19 files with the different naming convention\n",
    "    elif x == \"19\":\n",
    "        # Read mainland glacier file\n",
    "        mainland_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \\\n",
    "        \"-mainland-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(mainland_glacier_shapefile)\n",
    "        glacier_regions_part['reg_name'] = 'Antarctic Mainland'\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)\n",
    "        \n",
    "        # Read island glacier file\n",
    "        island_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \\\n",
    "        \"-islands-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(island_glacier_shapefile)\n",
    "        glacier_regions_part['reg_name'] = 'Subantarctic Islands'\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)  \n",
    "   \n",
    "    # Read all the other regions\n",
    "    else:\n",
    "        glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(glacier_shapefile_fn)\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating csv file: data/final-dataset/compiled-glacier-sizes.csv\n"
     ]
    }
   ],
   "source": [
    "# Write glacier dataframe to csv\n",
    "csv_glacier_fp = \"data/final-dataset/compiled-glacier-sizes.csv\"\n",
    "if os.path.exists(csv_glacier_fp) == False:\n",
    "    glacier_regions.to_csv(csv_glacier_fp, encoding='utf-8-sig', \n",
    "                           index=False, columns=['region_no', 'reg_name', 'glac_name', \n",
    "                                                 'glims_id', 'primeclass', 'area_km2', 'date'])\n",
    "    print(\"Creating csv file: \" + csv_glacier_fp)\n",
    "else:\n",
    "    print(csv_glacier_fp + \" already extists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the ice catchment shapefiles\n",
    "for x in region:\n",
    "    # Set up special case for Region 19 - Antarctica with the different naming convention for mainland and islands\n",
    "    # and for the inclusion of clipped ice caps\n",
    "    if x == \"19\":\n",
    "        # Unzip mainland ice cap file\n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-mainland-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "\n",
    "        # Unzip island ice cap file\n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-islands-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        # Unzip clipped file\n",
    "        if include_clipped == 1:\n",
    "            ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-islands-largest-complexes-clipped.zip\"\n",
    "        \n",
    "            with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "                zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "                \n",
    "    # Special statemnet for regions 5, 7, and 9 which have clipped versions of the ice caps\n",
    "    elif x == \"5\" or x == \"7\" or x == \"9\":\n",
    "        # Unzip regular ice cap files for these regions\n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        # Unzip the clipped ice cap files for these regions\n",
    "        if include_clipped == 1:\n",
    "            ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes-clipped.zip\"\n",
    "        \n",
    "            with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "                zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "                \n",
    "    # Unzip all other regions ice cap files\n",
    "    else: \n",
    "        ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes.zip\"\n",
    "        \n",
    "        with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized ice catchment shapefiles and concatenate them to a single data frame\n",
    "for x in region:\n",
    "    ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes.shp\"\n",
    "    \n",
    "    # Read first shapefile to set up dataframe\n",
    "    if x == \"1\":\n",
    "        ic_regions = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions['clipped'] = 0\n",
    "        \n",
    "    # Read region 19 shapefile\n",
    "    elif x == \"19\":\n",
    "        # Read mainland ice cap file\n",
    "        ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-mainland-largest-complexes.shp\"\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions_part['reg_name'] = 'Antarctic Mainland'\n",
    "        ic_regions = ic_regions.append(ic_regions_part)\n",
    "        \n",
    "        # Read island ice cap file\n",
    "        ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-islands-largest-complexes.shp\"\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions_part['reg_name'] = 'Subantarctic Islands'\n",
    "        ic_regions = ic_regions.append(ic_regions_part)\n",
    "        \n",
    "        # Read island clipped ice cap file\n",
    "        if include_clipped == 1:\n",
    "            ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-islands-largest-complexes-clipped.shp\"\n",
    "            ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "            ic_regions_part['clipped'] = 1\n",
    "            # drop rgi_ids column\n",
    "            #ic_regions_part = ic_regions_rgi.drop(['rgi_ids'], axis=1)\n",
    "            ic_regions = ic_regions.append(ic_regions_part)\n",
    "    \n",
    "    # Read regions 5, 7, and 9 and the clipped versions as well if desired\n",
    "    elif x == \"5\" or x == \"7\" or x == \"9\":\n",
    "        # Read the regular ice cap files for these regions\n",
    "        ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes.shp\"\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions = ic_regions.append(ic_regions_part)\n",
    "        \n",
    "        # Read the clipped versions of these ice cap files for these regions\n",
    "        if include_clipped == 1:\n",
    "            ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes-clipped.shp\"        \n",
    "            ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "            ic_regions_part['clipped'] = 1\n",
    "            ic_regions = ic_regions.append(ic_regions_part)\n",
    "\n",
    "    # Read all the other region shapefiles\n",
    "    else:\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions_part['clipped'] = 0\n",
    "        ic_regions = ic_regions.append(ic_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_no</th>\n",
       "      <th>reg_name</th>\n",
       "      <th>ic_name</th>\n",
       "      <th>primeclass</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>geometry</th>\n",
       "      <th>clipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>Southern Andes</td>\n",
       "      <td>Cordillera Darwin Icefield</td>\n",
       "      <td>2</td>\n",
       "      <td>1893.579143</td>\n",
       "      <td>2000-06-15</td>\n",
       "      <td>2007-09-06</td>\n",
       "      <td>(POLYGON ((-70.10073199999999 -54.740177, -70....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Tasman Glacier Complex</td>\n",
       "      <td>2</td>\n",
       "      <td>248.785404</td>\n",
       "      <td>1978-01-01</td>\n",
       "      <td>2009-02-17</td>\n",
       "      <td>(POLYGON ((170.1329175306282 -43.5884397947709...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Adams &amp; Lambert Glacier Complex</td>\n",
       "      <td>2</td>\n",
       "      <td>42.952788</td>\n",
       "      <td>1978-01-01</td>\n",
       "      <td>1978-01-01</td>\n",
       "      <td>POLYGON ((170.753766 -43.31865, 170.753756 -43...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Lyell &amp; Ramsay Glacier Complex</td>\n",
       "      <td>2</td>\n",
       "      <td>31.190311</td>\n",
       "      <td>1978-01-01</td>\n",
       "      <td>1978-01-01</td>\n",
       "      <td>POLYGON ((170.802738 -43.316774, 170.802939 -4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>Antarctic Mainland</td>\n",
       "      <td>Antarctic Penisula Glacier Complex</td>\n",
       "      <td>2</td>\n",
       "      <td>80851.900280</td>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>POLYGON ((-65.016774 -67.351969, -65.027069 -6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>Subantarctic Islands</td>\n",
       "      <td>Alexander Island Glacier Complex</td>\n",
       "      <td>2</td>\n",
       "      <td>47486.230388</td>\n",
       "      <td>1979-02-99</td>\n",
       "      <td>2001-12-20</td>\n",
       "      <td>POLYGON ((-68.65115005999996 -72.2207072749999...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>Subantarctic Islands</td>\n",
       "      <td>Thurston Island Ice Cap</td>\n",
       "      <td>3</td>\n",
       "      <td>11133.020670</td>\n",
       "      <td>1972-12-04</td>\n",
       "      <td>1972-12-04</td>\n",
       "      <td>POLYGON ((-96.38988303899998 -72.2732756079999...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region_no              reg_name                             ic_name  \\\n",
       "2        17        Southern Andes          Cordillera Darwin Icefield   \n",
       "0        18           New Zealand              Tasman Glacier Complex   \n",
       "1        18           New Zealand     Adams & Lambert Glacier Complex   \n",
       "2        18           New Zealand      Lyell & Ramsay Glacier Complex   \n",
       "0        19    Antarctic Mainland  Antarctic Penisula Glacier Complex   \n",
       "0        19  Subantarctic Islands    Alexander Island Glacier Complex   \n",
       "1        19  Subantarctic Islands             Thurston Island Ice Cap   \n",
       "\n",
       "   primeclass      area_km2    min_date    max_date  \\\n",
       "2           2   1893.579143  2000-06-15  2007-09-06   \n",
       "0           2    248.785404  1978-01-01  2009-02-17   \n",
       "1           2     42.952788  1978-01-01  1978-01-01   \n",
       "2           2     31.190311  1978-01-01  1978-01-01   \n",
       "0           2  80851.900280  2002-12-31  2002-12-31   \n",
       "0           2  47486.230388  1979-02-99  2001-12-20   \n",
       "1           3  11133.020670  1972-12-04  1972-12-04   \n",
       "\n",
       "                                            geometry  clipped  \n",
       "2  (POLYGON ((-70.10073199999999 -54.740177, -70....        0  \n",
       "0  (POLYGON ((170.1329175306282 -43.5884397947709...        0  \n",
       "1  POLYGON ((170.753766 -43.31865, 170.753756 -43...        0  \n",
       "2  POLYGON ((170.802738 -43.316774, 170.802939 -4...        0  \n",
       "0  POLYGON ((-65.016774 -67.351969, -65.027069 -6...        0  \n",
       "0  POLYGON ((-68.65115005999996 -72.2207072749999...        0  \n",
       "1  POLYGON ((-96.38988303899998 -72.2732756079999...        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic_regions[50:57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final-dataset/compiled-complex-sizes.csv already extists\n"
     ]
    }
   ],
   "source": [
    "# Write ice complex dataframe to csv\n",
    "csv_catchment_fp = \"data/final-dataset/compiled-complex-sizes.csv\"\n",
    "if os.path.exists(csv_catchment_fp) == False:\n",
    "    ic_regions.to_csv(csv_catchment_fp, encoding='utf-8-sig', index=False, columns=['region_no', 'reg_name', \n",
    "                                                                                    'ic_name', 'primeclass', \n",
    "                                                                                    'area_km2', 'min_date',\n",
    "                                                                                    'max_date', 'clipped'])\n",
    "                                                                                    \n",
    "    print(\"Creating csv file: \" + csv_catchment_fp)\n",
    "else:\n",
    "    print(csv_catchment_fp + \" already extists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unzipped files to save disk space\n",
    "filelist = glob.glob(\"data/final-dataset/unzipped/*\")\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
