{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Results\n",
    "\n",
    "This notebook reads the results from each of the finalized shapefiles and writes the results to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import zipfile\n",
    "\n",
    "# set working dir\n",
    "HOME = op.join(op.expanduser(\"~\"))\n",
    "os.chdir(os.path.join(HOME, \"git/wgms-glacier-project\"))\n",
    "\n",
    "# Set glacier and ice catchment region numbers\n",
    "region = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n",
    "catch_region = ['3', '4', '5', '6', '7', '8', '9', '10', '17', '19']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the glacier shapefiles\n",
    "for x in region:\n",
    "    # Read Region 19 separately because they have a different naming convention\n",
    "    if x == \"19\":\n",
    "        mainland_glacier_fp = \"data/final-dataset/region-\" + x + \"-mainland-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(mainland_glacier_fp,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "            \n",
    "        island_glacier_fp = \"data/final-dataset/region-\" + x + \"-islands-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(island_glacier_fp,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "    else:\n",
    "        glacier_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-glaciers.zip\"\n",
    "        with zipfile.ZipFile(glacier_zipfile_fn,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized glacier shapefiles and concatenate them to a single data frame\n",
    "for x in region:\n",
    "    if x == '1':\n",
    "        glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "        glacier_regions = gpd.read_file(glacier_shapefile_fn)\n",
    "\n",
    "    # Read Regioin 19 files\n",
    "    elif x == \"19\":\n",
    "        mainland_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-mainland-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(mainland_glacier_shapefile)\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)\n",
    "        \n",
    "        island_glacier_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-islands-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(island_glacier_shapefile)\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)  \n",
    "        \n",
    "    else:\n",
    "        glacier_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-glaciers.shp\"\n",
    "        glacier_regions_part = gpd.read_file(glacier_shapefile_fn)\n",
    "        glacier_regions = glacier_regions.append(glacier_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final-dataset/compiled-glacier-sizes.csv already extists\n"
     ]
    }
   ],
   "source": [
    "# Write glacier dataframe to csv\n",
    "csv_glacier_fp = \"data/final-dataset/compiled-glacier-sizes.csv\"\n",
    "if os.path.exists(csv_glacier_fp) == False:\n",
    "    glacier_regions.to_csv(csv_glacier_fp, encoding='utf-8-sig', \n",
    "                           index=False, columns=['region_no', 'reg_name', 'glac_name', \n",
    "                                                 'glims_id', 'primeclass', 'area_km2', 'date'])\n",
    "    print(\"Creating csv file: \" + csv_glacier_fp)\n",
    "else:\n",
    "    print(csv_glacier_fp + \" already extists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the ice catchment shapefiles\n",
    "for x in catch_region:\n",
    "    ic_zipfile_fn = \"data/final-dataset/region-\" + x + \"-largest-complexes.zip\"\n",
    "    with zipfile.ZipFile(ic_zipfile_fn,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "        \n",
    "# This section no longer needed becuase not spliting up region 7, may add it back for region 19 though. TBD\n",
    "#    if x == \"7\":\n",
    "#        # Unzip region 7 separately because its file naming convention is different\n",
    "#        jan_mayen_ic_fp = \"data/final-dataset/region-\" + x + \"-jan-mayen-largest-ice-caps.zip\"\n",
    "#        with zipfile.ZipFile(jan_mayen_ic_fp,\"r\") as zip_ref:\n",
    "#            zip_ref.extractall(\"data/final-dataset/unzipped\")\n",
    "#            \n",
    "#        svalbard_ic_fp = \"data/final-dataset/region-\" + x + \"-svalbard-largest-ice-caps.zip\"\n",
    "#        with zipfile.ZipFile(svalbard_ic_fp,\"r\") as zip_ref:\n",
    "#            zip_ref.extractall(\"data/final-dataset/unzipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open finalized ice catchment shapefiles and concatenate them to a single data frame\n",
    "for x in catch_region:\n",
    "    ic_shapefile_fn = \"data/final-dataset/unzipped/region-\" + x + \"-largest-complexes.shp\"\n",
    "    if x == \"3\":\n",
    "        ic_regions = gpd.read_file(ic_shapefile_fn)\n",
    "\n",
    "# This section no longer needed becuase not spliting up region 7, may add it back for region 19 though. TBD\n",
    "#    elif x == \"7\":\n",
    "#        jan_mayen_ic_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-jan-mayen-largest-ice-caps.shp\"\n",
    "#        ic_regions_part = gpd.read_file(jan_mayen_ic_shapefile)\n",
    "#        ic_regions = ic_regions.append(ic_regions_part)\n",
    "#        \n",
    "#        svalbard_ic_shapefile = \"data/final-dataset/unzipped/region-\" + x + \"-svalbard-largest-ice-caps.shp\"\n",
    "#        ic_regions_part = gpd.read_file(svalbard_ic_shapefile)\n",
    "#        ic_regions = ic_regions.append(ic_regions_part)  \n",
    "        \n",
    "    else:\n",
    "        ic_regions_part = gpd.read_file(ic_shapefile_fn)\n",
    "        ic_regions = ic_regions.append(ic_regions_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final-dataset/compiled-complex-sizes.csv already extists\n"
     ]
    }
   ],
   "source": [
    "# Write ice complex dataframe to csv\n",
    "csv_catchment_fp = \"data/final-dataset/compiled-complex-sizes.csv\"\n",
    "if os.path.exists(csv_catchment_fp) == False:\n",
    "    ic_regions.to_csv(csv_catchment_fp, encoding='utf-8-sig', index=False, columns=['region_no', 'reg_name', 'ic_name', \n",
    "                                                                                    'primeclass', 'area_km2', 'min_date', \n",
    "                                                                                    'max_date'])\n",
    "    print(\"Creating csv file: \" + csv_catchment_fp)\n",
    "else:\n",
    "    print(csv_catchment_fp + \" already extists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unzipped files to save disk space\n",
    "filelist = glob.glob(\"data/final-dataset/unzipped/*\")\n",
    "for f in filelist:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
