{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare GLIMS and RGI Data\n",
    "Author: Ann Windnagel\n",
    "\n",
    "Date: 3/10/19\n",
    "\n",
    "This notebook does a comparison of GLIMS and RGI data to determine the 10 largest glaciers in each of the 19 world glacier regions and saves those to csv files; one for each region for GLIMS and RGI for a total of 38 output files.\n",
    "\n",
    "Using those csv files, the 3 largest glaciers are selected from GLIMS and RGI and those are saved to a shapefile for each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# set working dir\n",
    "HOME = op.join(op.expanduser(\"~\"))\n",
    "os.chdir(os.path.join(HOME, \"git/wgms-glacier-project\"))\n",
    "\n",
    "# Set up path to load scripts\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import scripts.wgms_scripts as ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set region numbers\n",
    "region_no = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLIMS GLIMS GLIMS GLIMS\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIMS Region 1 largest 10 CSV file already exists\n",
      "GLIMS Region 2 largest 10 CSV file already exists\n",
      "GLIMS Region 3 largest 10 CSV file already exists\n",
      "GLIMS Region 4 largest 10 CSV file already exists\n",
      "GLIMS Region 5 largest 10 CSV file already exists\n",
      "GLIMS Region 6 largest 10 CSV file already exists\n",
      "GLIMS Region 7 largest 10 CSV file already exists\n",
      "GLIMS Region 8 largest 10 CSV file already exists\n",
      "GLIMS Region 9 largest 10 CSV file already exists\n",
      "GLIMS Region 10 largest 10 CSV file already exists\n",
      "GLIMS Region 11 largest 10 CSV file already exists\n",
      "GLIMS Region 12 largest 10 CSV file already exists\n",
      "GLIMS Region 13 largest 10 CSV file already exists\n",
      "GLIMS Region 14 largest 10 CSV file already exists\n",
      "GLIMS Region 15 largest 10 CSV file already exists\n",
      "GLIMS Region 16 largest 10 CSV file already exists\n",
      "GLIMS Region 17 largest 10 CSV file already exists\n",
      "GLIMS Region 18 largest 10 CSV file already exists\n",
      "GLIMS Region 19 largest 10 CSV file already exists\n"
     ]
    }
   ],
   "source": [
    "# Use the ten_largest function to create the 19 csv files for GLIMS\n",
    "for region in region_no:\n",
    "    glims_region_fp = \"data/glims/processed/cleaned/glims_region_\" + str(region) + \"_cleaned.shp\"\n",
    "    glims_polygons = gpd.read_file(glims_region_fp)\n",
    "    ws.ten_largest(glims_polygons, region, \"GLIMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGI RGI RGI RGI\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGI Region 1 largest 10 CSV file already exists\n",
      "RGI Region 2 largest 10 CSV file already exists\n",
      "RGI Region 3 largest 10 CSV file already exists\n",
      "RGI Region 4 largest 10 CSV file already exists\n",
      "RGI Region 5 largest 10 CSV file already exists\n",
      "RGI Region 6 largest 10 CSV file already exists\n",
      "RGI Region 7 largest 10 CSV file already exists\n",
      "RGI Region 8 largest 10 CSV file already exists\n",
      "RGI Region 9 largest 10 CSV file already exists\n",
      "RGI Region 10 largest 10 CSV file already exists\n",
      "RGI Region 11 largest 10 CSV file already exists\n",
      "RGI Region 12 largest 10 CSV file already exists\n",
      "RGI Region 13 largest 10 CSV file already exists\n",
      "RGI Region 14 largest 10 CSV file already exists\n",
      "RGI Region 15 largest 10 CSV file already exists\n",
      "RGI Region 16 largest 10 CSV file already exists\n",
      "RGI Region 17 largest 10 CSV file already exists\n",
      "RGI Region 18 largest 10 CSV file already exists\n",
      "RGI Region 19 largest 10 CSV file already exists\n"
     ]
    }
   ],
   "source": [
    "# Use the ten_largest function to create the 19 csv files for RGI\n",
    "for region in region_no:\n",
    "    #rgi_region_fp = \"data/rgi/processed/largest/rgi_region_\" + str(region) + \"_cleaned.shp\"\n",
    "    rgi_polygons = ws.open_rgi_region(region)\n",
    "    ws.ten_largest(rgi_polygons, region, \"RGI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 3 largest glaicers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLIMS GLIMS GLIMS GLIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_no:\n",
    "    # Open cleaned GLIMS shapefile for each region\n",
    "    glims_region_fp = \"data/glims/processed/cleaned/glims_region_\" + str(region) + \"_cleaned.shp\"\n",
    "    glims_polygons = gpd.read_file(glims_region_fp)\n",
    "\n",
    "    # Open GLIMS csv file with 10 largest glaciers\n",
    "    glims_largest_csv = ws.print_10_largest_glims(region, do_print='false')\n",
    "    \n",
    "    # Select 3 largest from GLIMS current region\n",
    "    glims_largest_name_1 = glims_largest_csv.iloc[0:1]\n",
    "    glims_largest_pd_1 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_1['glac_id'][0]]\n",
    "\n",
    "    glims_largest_name_2 = glims_largest_csv.iloc[1:2]\n",
    "    glims_largest_pd_2 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_2['glac_id'][1]]\n",
    "\n",
    "    glims_largest_name_3 = glims_largest_csv.iloc[2:3]\n",
    "    glims_largest_pd_3 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_3['glac_id'][2]]\n",
    "    \n",
    "    # Save 3 largest from GLIMS for Region 1 to shapefile\n",
    "    ws.save_3_largest(glims_largest_pd_1, glims_largest_pd_2, glims_largest_pd_3, region, 'GLIMS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
