{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare GLIMS and RGI Data\n",
    "Author: Ann Windnagel\n",
    "\n",
    "Date: 3/10/19\n",
    "\n",
    "This notebook does a comparison of GLIMS and RGI data to determine the 10 largest glaciers in each of the 19 world glacier regions and saves those to csv files; one for each region for GLIMS and RGI for a total of 38 output files.\n",
    "\n",
    "Using those csv files, the 5 largest glaciers are selected from GLIMS and RGI and those are saved to a shapefile for each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# set working dir\n",
    "HOME = op.join(op.expanduser(\"~\"))\n",
    "os.chdir(os.path.join(HOME, \"git/wgms-glacier-project\"))\n",
    "\n",
    "# Set up path to load scripts\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import scripts.wgms_scripts as ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set region numbers\n",
    "region_no = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLIMS GLIMS GLIMS GLIMS\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIMS Region 1 largest 10 CSV file already exists\n",
      "GLIMS Region 2 largest 10 CSV file already exists\n",
      "GLIMS Region 3 largest 10 CSV file already exists\n",
      "GLIMS Region 4 largest 10 CSV file already exists\n",
      "GLIMS Region 5 largest 10 CSV file already exists\n",
      "GLIMS Region 6 largest 10 CSV file already exists\n",
      "GLIMS Region 7 largest 10 CSV file already exists\n",
      "GLIMS Region 8 largest 10 CSV file already exists\n",
      "GLIMS Region 9 largest 10 CSV file already exists\n",
      "GLIMS Region 10 largest 10 CSV file already exists\n",
      "GLIMS Region 11 largest 10 CSV file already exists\n",
      "GLIMS Region 12 largest 10 CSV file already exists\n",
      "13\n",
      "GLIMS Region 14 largest 10 CSV file already exists\n",
      "GLIMS Region 15 largest 10 CSV file already exists\n",
      "GLIMS Region 16 largest 10 CSV file already exists\n",
      "GLIMS Region 17 largest 10 CSV file already exists\n",
      "GLIMS Region 18 largest 10 CSV file already exists\n",
      "GLIMS Region 19 largest 10 CSV file already exists\n"
     ]
    }
   ],
   "source": [
    "# Use the ten_largest function to create the 19 csv files for GLIMS\n",
    "for region in region_no:\n",
    "    glims_region_fp = \"data/glims/processed/cleaned/glims_region_\" + str(region) + \"_cleaned.shp\"\n",
    "    glims_polygons = gpd.read_file(glims_region_fp)\n",
    "    ws.ten_largest(glims_polygons, region, \"GLIMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGI RGI RGI RGI\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGI Region 1 largest 10 CSV file already exists\n",
      "RGI Region 2 largest 10 CSV file already exists\n",
      "RGI Region 3 largest 10 CSV file already exists\n",
      "RGI Region 4 largest 10 CSV file already exists\n",
      "RGI Region 5 largest 10 CSV file already exists\n",
      "RGI Region 6 largest 10 CSV file already exists\n",
      "RGI Region 7 largest 10 CSV file already exists\n",
      "RGI Region 8 largest 10 CSV file already exists\n",
      "RGI Region 9 largest 10 CSV file already exists\n",
      "RGI Region 10 largest 10 CSV file already exists\n",
      "RGI Region 11 largest 10 CSV file already exists\n",
      "RGI Region 12 largest 10 CSV file already exists\n",
      "RGI Region 13 largest 10 CSV file already exists\n",
      "RGI Region 14 largest 10 CSV file already exists\n",
      "RGI Region 15 largest 10 CSV file already exists\n",
      "RGI Region 16 largest 10 CSV file already exists\n",
      "RGI Region 17 largest 10 CSV file already exists\n",
      "RGI Region 18 largest 10 CSV file already exists\n",
      "RGI Region 19 largest 10 CSV file already exists\n"
     ]
    }
   ],
   "source": [
    "# Use the ten_largest function to create the 19 csv files for RGI\n",
    "for region in region_no:\n",
    "    #rgi_region_fp = \"data/rgi/processed/largest/rgi_region_\" + str(region) + \"_cleaned.shp\"\n",
    "    rgi_polygons = ws.open_rgi_region(region)\n",
    "    ws.ten_largest(rgi_polygons, region, \"RGI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 5 largest glaicers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLIMS GLIMS GLIMS GLIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/glims/processed/largest/glims_region_1_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_2_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_3_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_4_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_5_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_6_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_7_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_8_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_9_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_10_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_11_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_12_largest.shp file already exists\n",
      "Creating file data/glims/processed/largest/glims_region_13_largest.shp\n",
      "data/glims/processed/largest/glims_region_14_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_15_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_16_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_17_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_18_largest.shp file already exists\n",
      "data/glims/processed/largest/glims_region_19_largest.shp file already exists\n"
     ]
    }
   ],
   "source": [
    "for region in region_no:\n",
    "    # Open cleaned GLIMS shapefile for each region\n",
    "    glims_region_fp = \"data/glims/processed/cleaned/glims_region_\" + str(region) + \"_cleaned.shp\"\n",
    "    glims_polygons = gpd.read_file(glims_region_fp)\n",
    "\n",
    "    # Open GLIMS csv file with 10 largest glaciers\n",
    "    glims_largest_csv = ws.print_10_largest_glims(region, do_print='false')\n",
    "    \n",
    "    # Select 5 largest from GLIMS current region\n",
    "    glims_largest_name_1 = glims_largest_csv.iloc[0:1]\n",
    "    glims_largest_pd_1 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_1['glac_id'][0]]\n",
    "\n",
    "    glims_largest_name_2 = glims_largest_csv.iloc[1:2]\n",
    "    glims_largest_pd_2 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_2['glac_id'][1]]\n",
    "\n",
    "    glims_largest_name_3 = glims_largest_csv.iloc[2:3]\n",
    "    glims_largest_pd_3 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_3['glac_id'][2]]\n",
    "    \n",
    "    glims_largest_name_4 = glims_largest_csv.iloc[3:4]\n",
    "    glims_largest_pd_4 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_4['glac_id'][3]]\n",
    "    \n",
    "    glims_largest_name_5 = glims_largest_csv.iloc[4:5]\n",
    "    glims_largest_pd_5 = glims_polygons[glims_polygons['glac_id']==glims_largest_name_5['glac_id'][4]]    \n",
    "    \n",
    "    # Save 5 largest from GLIMS for current region to shapefile\n",
    "    ws.save_5_largest(glims_largest_pd_1, glims_largest_pd_2, glims_largest_pd_3, \n",
    "                      glims_largest_pd_4, glims_largest_pd_5, region, 'GLIMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGI RGI RGI RGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/rgi/processed/largest/rgi_region_1_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_2_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_3_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_4_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_5_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_6_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_7_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_8_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_9_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_10_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_11_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_12_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_13_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_14_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_15_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_16_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_17_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_18_largest.shp file already exists\n",
      "data/rgi/processed/largest/rgi_region_19_largest.shp file already exists\n"
     ]
    }
   ],
   "source": [
    "for region in region_no:\n",
    "    # Open RGI regional shapefile\n",
    "    rgi_polygons = ws.open_rgi_region(region)\n",
    "\n",
    "    # Open RGI Region csv file with 10 largest glaciers\n",
    "    rgi_largest = ws.print_10_largest_rgi(region, do_print='false')\n",
    "    \n",
    "    # Select 5 largest from RGI Region\n",
    "    rgi_largest_name_1 = rgi_largest.iloc[0:1]\n",
    "    rgi_largest_pd_1 = rgi_polygons[rgi_polygons['GLIMSId']==rgi_largest_name_1['GLIMSId'][0]]\n",
    "\n",
    "    rgi_largest_name_2 = rgi_largest.iloc[1:2]\n",
    "    rgi_largest_pd_2 = rgi_polygons[rgi_polygons['GLIMSId']==rgi_largest_name_2['GLIMSId'][1]]\n",
    "\n",
    "    rgi_largest_name_3 = rgi_largest.iloc[2:3]\n",
    "    rgi_largest_pd_3 = rgi_polygons[rgi_polygons['GLIMSId']==rgi_largest_name_3['GLIMSId'][2]]\n",
    "    \n",
    "    rgi_largest_name_4 = rgi_largest.iloc[3:4]\n",
    "    rgi_largest_pd_4 = rgi_polygons[rgi_polygons['GLIMSId']==rgi_largest_name_4['GLIMSId'][3]]\n",
    "    \n",
    "    rgi_largest_name_5 = rgi_largest.iloc[4:5]\n",
    "    rgi_largest_pd_5 = rgi_polygons[rgi_polygons['GLIMSId']==rgi_largest_name_5['GLIMSId'][4]]\n",
    "    \n",
    "    # Save 5 largest from RGI for current region to shapefile\n",
    "    ws.save_5_largest(rgi_largest_pd_1, rgi_largest_pd_2, rgi_largest_pd_3, \n",
    "                      rgi_largest_pd_4, rgi_largest_pd_5, region, 'RGI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RGI Jan Mayen Region 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/rgi/processed/largest/rgi_region_7_jan_mayen_largest.shp already exists\n"
     ]
    }
   ],
   "source": [
    "# For RGI want to also save the 3 largest glaciers in Jan Mayen (Region 7)\n",
    "# Only need to save 3 largest for this area (not 5)\n",
    "# The largest glaciers in Region 7 are in Svalbard but want to do a separate analysis of Jan Mayen\n",
    "\n",
    "# Open RGI Region 7 glacier files\n",
    "rgi_polygons = ws.open_rgi_region(7)\n",
    "\n",
    "# Create a clipping polygon for Jan Mayan for plotting the Jan Mayen glaciers\n",
    "# Create dataframe that holds the clipping box\n",
    "jan_mayen_points = Polygon([(-9.5691, 71.5205), (-7.2620, 71.5205),\n",
    "                                 (-7.2620, 70.5136), (-9.5691, 70.5136), \n",
    "                                 (-9.5691, 71.5205)])\n",
    "jan_mayen_gdf = gpd.GeoDataFrame([1],\n",
    "                                 geometry=[jan_mayen_points],\n",
    "                                 crs={'init': 'epsg:4362'})\n",
    "\n",
    "# Find the RGI outlines that lie within the jan mayen outline\n",
    "jan_mayen_pip_mask = ws.pip(rgi_polygons, jan_mayen_gdf)\n",
    "\n",
    "# Pass pip_mask into data to get the ones that are in the specified region\n",
    "jan_mayen_region = rgi_polygons.loc[jan_mayen_pip_mask]\n",
    "\n",
    "# Select the 3 largest jan mayen glaciers\n",
    "jan_mayen_largest_df = jan_mayen_region[['RGIId', 'GLIMSId', 'Area', 'Name', 'BgnDate', 'geometry']].nlargest(3, 'Area')\n",
    "\n",
    "# Save 3 largest from specified region to shapefile\n",
    "largest_jm_3_fp = \"data/rgi/processed/largest/rgi_region_7_jan_mayen_largest.shp\"\n",
    "if os.path.exists(largest_jm_3_fp) == False:\n",
    "    print(\"Creating file \" + largest_jm_3_fp)\n",
    "    jan_mayen_largest_df.to_file(driver='ESRI Shapefile', filename=largest_jm_3_fp)\n",
    "else:\n",
    "    print(largest_jm_3_fp + \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/rgi/processed/largest/rgi_region_7_jan_mayen_largest.csv already exists\n"
     ]
    }
   ],
   "source": [
    "# Save the ten largest to csv file\n",
    "ten_jm_largest_df = jan_mayen_region[['GLIMSId', 'Area', 'Name', 'BgnDate']].nlargest(10, 'Area')\n",
    "\n",
    "# Save to csv file if it doesn't already exist\n",
    "rgi_jm_largest_csv_fp = \"data/rgi/processed/largest/rgi_region_7_jan_mayen_largest.csv\"\n",
    "if os.path.exists(rgi_jm_largest_csv_fp) == False:\n",
    "    print(\"Creating file \" + rgi_jm_largest_csv_fp)\n",
    "    ten_jm_largest_df.to_csv(rgi_jm_largest_csv_fp, index=False)\n",
    "else:\n",
    "    print(rgi_jm_largest_csv_fp + \" already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
